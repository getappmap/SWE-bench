name: Run the benchmark

on:
  workflow_dispatch:
    inputs:
      filter:
        description: "Instance filter regex"
        required: true
        default: "django-1[67]"
      dataset:
        description: "Dataset name"
        required: true
        default: princeton-nlp/SWE-bench_Lite
      split:
        description: "Dataset split"
        required: true
        default: test
      retries:
        description: "Number of retries to perform on each instance until a patch is found"
        required: false
        default: "3"
      appmaps:
        description: "Set to true to use AppMaps"
        required: false

  pull_request:
    types: [opened, synchronize, labeled]

jobs:
  build-appmap-js:
    if: ${{ contains(github.event.pull_request.labels.*.name, 'evaluate') || github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      # Cache the appmap-js build
      - name: Cache appmap-js build
        uses: actions/cache@v4
        id: cache-appmap-js
        with:
          lookup-only: true
          path: |
            submodules/appmap-js/node_modules
            submodules/appmap-js/packages/*/built
            submodules/appmap-js/packages/*/dist
            submodules/appmap-js/packages/*/node_modules
          key: appmap-js-dist-${{ runner.os }}-${{ hashFiles('.git/modules/submodules/appmap-js/refs/heads') }}

      - name: Set up Node.js
        if: steps.cache-appmap-js.outputs.cache-hit != 'true'
        uses: actions/setup-node@v3

      - name: Build submodules
        if: steps.cache-appmap-js.outputs.cache-hit != 'true'
        env:
          PUPPETEER_SKIP_DOWNLOAD: true
        run: |
          cd submodules/appmap-js
          git checkout -- .
          yarn
          yarn build
          chmod +x packages/cli/built/cli.js

  solve:
    needs: build-appmap-js
    if: ${{ contains(github.event.pull_request.labels.*.name, 'evaluate') || github.event_name == 'workflow_dispatch' }}
    runs-on: swe-bench-ubuntu-latest
    defaults:
      run:
        shell: bash -leo pipefail {0}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          submodules: true

      # Cache the conda environment
      - name: Cache conda environment
        id: cache-conda
        uses: actions/cache@v4
        with:
          path: /usr/share/miniconda/envs/swe-bench
          key: conda-${{ runner.os }}-${{ hashFiles('environment.yml') }}

      # Create conda env if cache miss happens
      - name: Create conda env
        if: steps.cache-conda.outputs.cache-hit != 'true'
        run: |
          conda init bash
          conda env create -f environment.yml

      # Restore the appmap-js build
      - name: Restore appmap-js build
        uses: actions/cache/restore@v4
        id: cache-appmap-js
        with:
          fail-on-cache-miss: true
          path: |
            submodules/appmap-js/node_modules
            submodules/appmap-js/packages/*/built
            submodules/appmap-js/packages/*/dist
            submodules/appmap-js/packages/*/node_modules
          key: appmap-js-dist-${{ runner.os }}-${{ hashFiles('.git/modules/submodules/appmap-js/refs/heads') }}

      - name: Run benchmark
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          SWE_DATASET: ${{ inputs.dataset }}
          SWE_SPLIT: ${{ inputs.split }}
          SWE_FILTER: ${{ inputs.filter }}
          SWE_RETRIES: ${{ inputs.retries }}
        run: |
          source /usr/share/miniconda/etc/profile.d/conda.sh
          conda activate swe-bench
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python appmap/solve.py \
            --instances "${SWE_DATASET:-princeton-nlp/SWE-bench_Lite}" \
            --split ${SWE_SPLIT:-test} \
            --filter "${SWE_FILTER:-django-1[67]}" \
            --retries "${SWE_RETRIES:-3}" \
            --appmap_command $(pwd)/submodules/appmap-js/packages/cli/built/cli.js \
            --lint_command "flake8 --extend-ignore=BLK100,C402,C408,C416,D,E122,E124,E127,E128,E201,E202,E221,E225,E231,E251,E261,E265,E266,E302,E303,E305,E402,E501,E502,E713,E731,F401,F841,W293" \
            --temp_dir "${{ runner.temp }}" \
            --num_workers 6 \
            --path_conda $(conda info --base) \
            --verbose \
            $(test -n "${{ inputs.appmaps }}" && echo --appmaps)

      - name: Run evaluation
        env:
          SWE_DATASET: ${{ inputs.dataset }}
        run: |
          mkdir -p logs
          source /usr/share/miniconda/etc/profile.d/conda.sh
          conda activate swe-bench
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          python swebench/harness/run_evaluation.py \
            --predictions_path predictions.jsonl \
            --swe_bench_tasks "${SWE_DATASET:-princeton-nlp/SWE-bench_Lite}" \
            --log_dir logs \
            --testbed "${{ runner.temp }}" \
            --skip_existing \
            --timeout 900 \
            --verbose \
            --num_processes 8 \
            --path_conda $(conda info --base)

      - name: Generate AppMap report
        if: always()
        env:
          SWE_DATASET: ${{ inputs.dataset }}
          SWE_SPLIT: ${{ inputs.split }}
        run: |
          source /usr/share/miniconda/etc/profile.d/conda.sh
          conda activate swe-bench
          export PYTHONPATH=$PYTHONPATH:$(pwd)
          conda info
          python appmap/report.py \
            --instances "${SWE_DATASET:-princeton-nlp/SWE-bench_Lite}" \
            --split ${SWE_SPLIT:-test}

      - name: Archive predictions and logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results
          path: |
            logs/
            predictions.jsonl
            results.csv
